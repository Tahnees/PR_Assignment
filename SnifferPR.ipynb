{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tahnees/PR_Assignment/blob/main/SnifferPR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/News_clippings.zip\"\n",
        "extract_to = \"/content/visual_news_dataset\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n"
      ],
      "metadata": {
        "id": "gqebsimixPYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(extract_to):\n",
        "    print(f\"{root}:\\n  Dirs: {dirs}\\n  Files: {files[:5]}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mgbSvRizbkY",
        "outputId": "21bc7f48-d2cf-4acf-8694-b3e25a987aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/visual_news_dataset:\n",
            "  Dirs: ['News_clippings']\n",
            "  Files: []\n",
            "\n",
            "/content/visual_news_dataset/News_clippings:\n",
            "  Dirs: ['data', 'embeddings']\n",
            "  Files: []\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/data:\n",
            "  Dirs: ['scene_resnet_place', 'merged_balanced', 'semantics_clip_text_image', 'semantics_clip_text_text', 'person_sbert_text_text', 'full']\n",
            "  Files: []\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/data/scene_resnet_place:\n",
            "  Dirs: []\n",
            "  Files: ['test.json', 'train.json', 'val.json']\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/data/merged_balanced:\n",
            "  Dirs: []\n",
            "  Files: ['test.json', 'train.json', 'val.json']\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/data/semantics_clip_text_image:\n",
            "  Dirs: []\n",
            "  Files: ['test.json', 'train.json', 'val.json']\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/data/semantics_clip_text_text:\n",
            "  Dirs: []\n",
            "  Files: ['test.json', 'train.json', 'val.json']\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/data/person_sbert_text_text:\n",
            "  Dirs: []\n",
            "  Files: ['test.json', 'train.json', 'val.json']\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/data/full:\n",
            "  Dirs: []\n",
            "  Files: ['train.json']\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/embeddings:\n",
            "  Dirs: ['facenet_embeddings', 'clip_text_embeddings', 'sbert_embeddings', 'places_resnet50', 'clip_image_embeddings']\n",
            "  Files: []\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/embeddings/facenet_embeddings:\n",
            "  Dirs: []\n",
            "  Files: ['facenet_embeddings_test.pkl', 'facenet_embeddings_val.pkl', 'facenet_embeddings_train.pkl']\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/embeddings/clip_text_embeddings:\n",
            "  Dirs: []\n",
            "  Files: ['clip_text_embeddings_val.pkl', 'clip_text_embeddings_test.pkl', 'clip_text_embeddings_train.pkl']\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/embeddings/sbert_embeddings:\n",
            "  Dirs: []\n",
            "  Files: ['sbert_embeddings_val.pkl', 'sbert_embeddings_test.pkl', 'sbert_embeddings_train.pkl']\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/embeddings/places_resnet50:\n",
            "  Dirs: []\n",
            "  Files: ['places_resnet50_test.pkl', 'places_resnet50_train.pkl', 'places_resnet50_val.pkl']\n",
            "\n",
            "/content/visual_news_dataset/News_clippings/embeddings/clip_image_embeddings:\n",
            "  Dirs: []\n",
            "  Files: ['clip_image_embeddings_train.pkl', 'clip_image_embeddings_val.pkl', 'clip_image_embeddings_test.pkl']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "\n",
        "train_path = \"/content/visual_news_dataset/News_clippings/data/semantics_clip_text_image/train.json\"\n",
        "limited_train_path = \"/content/visual_news_dataset/News_clippings/data/semantics_clip_text_image/train_5k.json\"\n",
        "\n",
        "\n",
        "with open(train_path, 'r') as f:\n",
        "    full_data = json.load(f)\n",
        "\n",
        "annotations = full_data[\"annotations\"]\n",
        "print(f\"Total samples: {len(annotations)}\")\n",
        "\n",
        "sampled_data = annotations\n",
        "\n",
        "with open(limited_train_path, 'w') as f:\n",
        "    json.dump({\"annotations\": sampled_data}, f, indent=2)\n",
        "\n",
        "print(\"Example entry:\\n\", sampled_data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hi_COH41fcf",
        "outputId": "a2da139f-6307-4256-92d0-50295b729dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 453128\n",
            "Example entry:\n",
            " {'id': 728421, 'image_id': 728421, 'similarity_score': 1, 'falsified': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ytcRx0XRIKF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "full_path = \"/content/visual_news_dataset/News_clippings/data/semantics_clip_text_image/train.json\"\n",
        "\n",
        "with open(full_path, 'r') as f:\n",
        "    full_data = json.load(f)\n",
        "\n",
        "all_samples = full_data[\"annotations\"]\n",
        "\n",
        "labels = [int(sample[\"falsified\"]) for sample in all_samples]\n",
        "train_data, val_data = train_test_split(all_samples, test_size=0.1, random_state=42, stratify=labels)\n",
        "\n",
        "with open(\"/content/train_clean.json\", \"w\") as f:\n",
        "    json.dump({\"annotations\": train_data}, f, indent=2)\n",
        "\n",
        "with open(\"/content/val_clean.json\", \"w\") as f:\n",
        "    json.dump({\"annotations\": val_data}, f, indent=2)\n",
        "\n",
        "print(f\"Train samples: {len(train_data)}\")\n",
        "print(f\"Val samples: {len(val_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp4HLK2NZyaf",
        "outputId": "00e12195-4762-4e44-878c-365266bdd49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 407815\n",
            "Val samples: 45313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "TRAIN_JSON = \"/content/train_clean.json\"\n",
        "VAL_JSON = \"/content/val_clean.json\"\n",
        "\n",
        "EMBED_PATHS = {\n",
        "    \"clip_image\": \"/content/visual_news_dataset/News_clippings/embeddings/clip_image_embeddings/clip_image_embeddings_train.pkl\",\n",
        "    \"clip_text\": \"/content/visual_news_dataset/News_clippings/embeddings/clip_text_embeddings/clip_text_embeddings_train.pkl\",\n",
        "    \"sbert\": \"/content/visual_news_dataset/News_clippings/embeddings/sbert_embeddings/sbert_embeddings_train.pkl\"\n",
        "}\n",
        "\n",
        "VAL_EMBED_PATHS = {\n",
        "    \"clip_image\": \"/content/visual_news_dataset/News_clippings/embeddings/clip_image_embeddings/clip_image_embeddings_val.pkl\",\n",
        "    \"clip_text\": \"/content/visual_news_dataset/News_clippings/embeddings/clip_text_embeddings/clip_text_embeddings_val.pkl\",\n",
        "    \"sbert\": \"/content/visual_news_dataset/News_clippings/embeddings/sbert_embeddings/sbert_embeddings_val.pkl\"\n",
        "}\n",
        "\n",
        "EXPECTED_SIZES = {\n",
        "    \"clip_image\": 512,\n",
        "    \"clip_text\": 512,\n",
        "    \"sbert\": 768\n",
        "}\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LR = 1e-3\n",
        "\n",
        "class SnifferEmbedDataset(Dataset):\n",
        "    def __init__(self, json_path, embed_paths):\n",
        "        with open(json_path, 'r') as f:\n",
        "            self.data = json.load(f)[\"annotations\"]\n",
        "\n",
        "        self.embeddings = {}\n",
        "        for name, path in embed_paths.items():\n",
        "            with open(path, 'rb') as f:\n",
        "                self.embeddings[name] = pickle.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        eid = entry[\"id\"]\n",
        "        label = int(entry.get(\"falsified\", False))\n",
        "        caption = entry.get(\"caption\", \"\")\n",
        "\n",
        "        vecs = {}\n",
        "        feature_parts = []\n",
        "        for name in self.embeddings:\n",
        "            expected_dim = EXPECTED_SIZES[name]\n",
        "            vec = self.embeddings[name].get(eid)\n",
        "            if vec is None:\n",
        "                vec = np.zeros(expected_dim, dtype=np.float32)\n",
        "            else:\n",
        "                vec = np.array(vec, dtype=np.float32)\n",
        "                if vec.shape[0] != expected_dim:\n",
        "                    vec = np.pad(vec, (0, expected_dim - vec.shape[0]))\n",
        "            vecs[name] = vec\n",
        "            feature_parts.append(vec)\n",
        "\n",
        "        feature_vector = np.concatenate(feature_parts)\n",
        "        features = torch.tensor(feature_vector, dtype=torch.float32)\n",
        "        return features, torch.tensor(label, dtype=torch.long), caption, eid, vecs\n",
        "\n",
        "\n",
        "class SnifferEmbedModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "def train_model(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    for features, labels, *_ in tqdm(dataloader):\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "        outputs = model(features)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for features, labels, *_ in dataloader:\n",
        "            features = features.to(device)\n",
        "            outputs = model(features)\n",
        "            preds = outputs.argmax(dim=1).cpu().tolist()\n",
        "            y_pred.extend(preds)\n",
        "            y_true.extend(labels.tolist())\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    print(f\"\\n Accuracy: {acc:.4f}\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred, target_names=[\"Real\", \"Out-of-context\"], zero_division=0))\n",
        "    return y_true, y_pred\n",
        "\n",
        "\n",
        "def explain_predictions(model, dataset, device, num_samples=5):\n",
        "    model.eval()\n",
        "    print(\"\\n Sample Paper-style Explanations:\\n\")\n",
        "    count = 0\n",
        "    for i in range(len(dataset)):\n",
        "        features, label, caption, eid, vecs = dataset[i]\n",
        "        features = features.unsqueeze(0).to(device)\n",
        "        pred = model(features).argmax().item()\n",
        "\n",
        "        label_text = \"Out-of-context\" if label.item() == 1 else \"Real\"\n",
        "        pred_text = \"Out-of-context\" if pred == 1 else \"Real\"\n",
        "\n",
        "        clip_sim = 1 - cosine(vecs[\"clip_image\"], vecs[\"clip_text\"])\n",
        "        sbert_score = np.linalg.norm(vecs[\"sbert\"])\n",
        "\n",
        "        if pred_text == \"Out-of-context\":\n",
        "            explanation = (\n",
        "                f\"No, the image is wrongly used in a different news context. \"\n",
        "                f\"On one hand, the caption discusses a specific context, but the person or scene in the image may not correspond. \"\n",
        "                f\"The similarity between the caption and image is low (cosine={clip_sim:.2f}), and the contextual coherence score is weak (sbert norm={sbert_score:.2f}). \"\n",
        "                f\"Therefore, the image is more likely to be misleading in this captioned context.\"\n",
        "            )\n",
        "        else:\n",
        "            explanation = (\n",
        "                f\"Yes, the image and caption likely match. The similarity score is high (cosine={clip_sim:.2f}) and the contextual coherence (sbert norm={sbert_score:.2f}) indicates consistency between the scene and text.\"\n",
        "            )\n",
        "\n",
        "        print(f\" ID: {eid}\")\n",
        "        print(f\" Caption: {caption}\")\n",
        "        print(f\" Prediction: {pred_text} | Ground Truth: {label_text}\")\n",
        "        print(f\" Explanation: {explanation}\\n\")\n",
        "\n",
        "        count += 1\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "\n",
        "input_dim = sum(EXPECTED_SIZES.values())\n",
        "train_set = SnifferEmbedDataset(TRAIN_JSON, EMBED_PATHS)\n",
        "val_set = SnifferEmbedDataset(VAL_JSON, VAL_EMBED_PATHS)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SnifferEmbedModel(input_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n Epoch {epoch+1}/{EPOCHS}\")\n",
        "    train_model(model, train_loader, optimizer, loss_fn)\n",
        "    evaluate_model(model, val_loader)\n",
        "\n",
        "explain_predictions(model, val_set, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCjl4dhDn4Hp",
        "outputId": "f2cb9d78-641c-4dfe-d6f0-0a9ef62d6de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12745/12745 [00:47<00:00, 266.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy: 0.5000\n",
            "[[22657     0]\n",
            " [22656     0]]\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          Real       0.50      1.00      0.67     22657\n",
            "Out-of-context       0.00      0.00      0.00     22656\n",
            "\n",
            "      accuracy                           0.50     45313\n",
            "     macro avg       0.25      0.50      0.33     45313\n",
            "  weighted avg       0.25      0.50      0.33     45313\n",
            "\n",
            "\n",
            " Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12745/12745 [00:47<00:00, 269.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy: 0.5000\n",
            "[[    0 22657]\n",
            " [    0 22656]]\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          Real       0.00      0.00      0.00     22657\n",
            "Out-of-context       0.50      1.00      0.67     22656\n",
            "\n",
            "      accuracy                           0.50     45313\n",
            "     macro avg       0.25      0.50      0.33     45313\n",
            "  weighted avg       0.25      0.50      0.33     45313\n",
            "\n",
            "\n",
            " Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12745/12745 [00:47<00:00, 268.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy: 0.5000\n",
            "[[22657     0]\n",
            " [22656     0]]\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          Real       0.50      1.00      0.67     22657\n",
            "Out-of-context       0.00      0.00      0.00     22656\n",
            "\n",
            "      accuracy                           0.50     45313\n",
            "     macro avg       0.25      0.50      0.33     45313\n",
            "  weighted avg       0.25      0.50      0.33     45313\n",
            "\n",
            "\n",
            " Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12745/12745 [00:47<00:00, 270.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy: 0.5000\n",
            "[[22657     0]\n",
            " [22656     0]]\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          Real       0.50      1.00      0.67     22657\n",
            "Out-of-context       0.00      0.00      0.00     22656\n",
            "\n",
            "      accuracy                           0.50     45313\n",
            "     macro avg       0.25      0.50      0.33     45313\n",
            "  weighted avg       0.25      0.50      0.33     45313\n",
            "\n",
            "\n",
            " Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12745/12745 [00:46<00:00, 274.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy: 0.5000\n",
            "[[22657     0]\n",
            " [22656     0]]\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          Real       0.50      1.00      0.67     22657\n",
            "Out-of-context       0.00      0.00      0.00     22656\n",
            "\n",
            "      accuracy                           0.50     45313\n",
            "     macro avg       0.25      0.50      0.33     45313\n",
            "  weighted avg       0.25      0.50      0.33     45313\n",
            "\n",
            "\n",
            " Sample Paper-style Explanations:\n",
            "\n",
            " ID: 1466565\n",
            " Caption: \n",
            " Prediction: Real | Ground Truth: Real\n",
            " Explanation: Yes, the image and caption likely match. The similarity score is high (cosine=nan) and the contextual coherence (sbert norm=0.00) indicates consistency between the scene and text.\n",
            "\n",
            " ID: 203211\n",
            " Caption: \n",
            " Prediction: Real | Ground Truth: Out-of-context\n",
            " Explanation: Yes, the image and caption likely match. The similarity score is high (cosine=nan) and the contextual coherence (sbert norm=0.00) indicates consistency between the scene and text.\n",
            "\n",
            " ID: 94787\n",
            " Caption: \n",
            " Prediction: Real | Ground Truth: Real\n",
            " Explanation: Yes, the image and caption likely match. The similarity score is high (cosine=nan) and the contextual coherence (sbert norm=0.00) indicates consistency between the scene and text.\n",
            "\n",
            " ID: 1626083\n",
            " Caption: \n",
            " Prediction: Real | Ground Truth: Out-of-context\n",
            " Explanation: Yes, the image and caption likely match. The similarity score is high (cosine=nan) and the contextual coherence (sbert norm=0.00) indicates consistency between the scene and text.\n",
            "\n",
            " ID: 986493\n",
            " Caption: \n",
            " Prediction: Real | Ground Truth: Out-of-context\n",
            " Explanation: Yes, the image and caption likely match. The similarity score is high (cosine=nan) and the contextual coherence (sbert norm=0.00) indicates consistency between the scene and text.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/spatial/distance.py:685: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  dist = 1.0 - uv / math.sqrt(uu * vv)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1hoQlg0vSFBfJVrkB3FB_ZgqX9naHSP6J",
      "authorship_tag": "ABX9TyNSjdKPW4u7sOxyLAHQ3W1W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}